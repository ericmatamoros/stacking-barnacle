{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from scipy.spatial.distance import cdist\n",
    "from itertools import combinations, product\n",
    "import os, fnmatch\n",
    "from biopandas.pdb import PandasPdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/eric1999/Desktop/Eric/'\n",
    "os. chdir(path)\n",
    "\n",
    "#Paths of the ML models and where to read the pdbs and store them\n",
    "pathML = '/Users/eric1999/Desktop/ML-stacking-noh_Eric/caca/'\n",
    "pathPDB = '/Users/eric1999/Desktop/Eric/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define functions\n",
    "def potency2(X):\n",
    "#    X3 = [ 1/(x**3) for x in X ]\n",
    "    X6 = [ 1/(x**6) for x in X ]\n",
    "#    X9 = [ 1/(x**9) for x in X ]\n",
    "#    X10 = [ 1/(x**10) for x in X ]\n",
    "    X12 = [ 1/(x**12) for x in X ]\n",
    "    X = [ 1/x for x in X ]\n",
    "#    out = np.array([X + X3 + X6 + X9 + X10 + X12])\n",
    "    out = np.array([X + X6 + X12])\n",
    "    return out\n",
    "\n",
    "def parsexyz(pdbfile, path):\n",
    "    x = {}\n",
    "    y = {}\n",
    "    z = {}\n",
    "    count = 0\n",
    "    for line in open(os.path.join(os.path.abspath(path), pdbfile)):\n",
    "        splited = line.split()\n",
    "        id = splited[0]\n",
    "        if id == \"ATOM\":\n",
    "            x[count] = float(splited[6])\n",
    "            y[count] = float(splited[7])\n",
    "            z[count] = float(splited[8])\n",
    "            count = count + 1\n",
    "    #Convert into dataframe\n",
    "    df = {'x':x, 'y':y, 'z':z}\n",
    "    df = pd.DataFrame(data=df)\n",
    "    return df\n",
    "\n",
    "def parsepdb(pdbfile, path):\n",
    "    eleno = {}\n",
    "    elety = {}\n",
    "    resid = {}\n",
    "    resno = {}\n",
    "    x = {}\n",
    "    y = {}\n",
    "    z = {}\n",
    "    B_factor = {}\n",
    "    count = 0\n",
    "    for line in open(os.path.join(os.path.abspath(path), pdbfile)):\n",
    "        splited = line.split()\n",
    "        id = splited[0]\n",
    "        if id == \"ATOM\":\n",
    "            eleno[count] = int(splited[1])\n",
    "            elety[count] = splited[2]\n",
    "            resid[count] = splited[3]\n",
    "            resno[count] = int(splited[5])\n",
    "            x[count] = float(splited[6])\n",
    "            y[count] = float(splited[7])\n",
    "            z[count] = float(splited[8])\n",
    "            B_factor[count] = float(splited[9])\n",
    "            count = count + 1\n",
    "    #Convert into dataframe\n",
    "    df = {'eleno': eleno, 'elety': elety, 'resid': resid, 'resno':resno, 'x':x, 'y':y, 'z':z, 'B_factor':B_factor}\n",
    "    df = pd.DataFrame(data=df)\n",
    "    df['resid'] = df['resid'].replace(\"5\", \"\", regex=True)\n",
    "    df['resid'] = df['resid'].replace(\"3\", \"\", regex=True)\n",
    "    return df\n",
    "\n",
    "#Generate dataframe and bind energies\n",
    "\n",
    "def scoring_backbone(top, energies, pdb):\n",
    "    df = pd.DataFrame(top[0],columns=['Atom1', 'Atom2'])\n",
    "    df[\"Energies\"] = energies\n",
    "\n",
    "    #Generate identifiers\n",
    "    Identifiers = np.unique(list(df[\"Atom1\"]) +  list(df[\"Atom2\"]))\n",
    "    \n",
    "    #Sum the energies per base\n",
    "    TotalEnergiesPerBase = []\n",
    "\n",
    "    for i in Identifiers:\n",
    "        Summed = sum(df[(df[\"Atom1\"] == i) | (df[\"Atom2\"] == i)][\"Energies\"])\n",
    "        TotalEnergiesPerBase.append(Summed)\n",
    "    \n",
    "    d = {'Atoms':Identifiers,'TotalEnergies':TotalEnergiesPerBase}\n",
    "    df_Final = pd.DataFrame(d)\n",
    "    \n",
    "    normalized = (df_Final[\"TotalEnergies\"] - min(df_Final[\"TotalEnergies\"])) / (max(df_Final[\"TotalEnergies\"]) - min(df_Final[\"TotalEnergies\"]))\n",
    "    \n",
    "    df_Final[\"TotalEnergies\"] = normalized\n",
    "    \n",
    "    for i in df_Final[\"Atoms\"]:\n",
    "        energy_rep = [df_Final[df_Final[\"Atoms\"] == i][\"TotalEnergies\"]] * len(pdb.loc[pdb[\"resno\"] == i, \"B_factor\"])\n",
    "        pdb.loc[pdb[\"resno\"] == i, \"B_factor\"] = energy_rep\n",
    "    \n",
    "    return(pdb)\n",
    "\n",
    "def topology(pdbfile, path):\n",
    "    df = parsepdb(pdbfile=pdbfile, path=path)\n",
    "    ## Get data for C1' atom of all residues\n",
    "    mindf = df[df.elety == \"C4\"]\n",
    "    ## Get combinations of all against all atoms\n",
    "    combs_close = list(combinations(mindf.resno, 2))\n",
    "    drop = df['elety'].isin(to_drop)\n",
    "    df2 = df[~drop]\n",
    "    df2.index = range(0, len(df2.index))\n",
    "    sorterA=[\"C1'\", \"N9\", \"C8\", \"N7\", \"C5\", \"C6\", \"N6\",\"N1\", \"C2\", \"N3\", \"C4\"]\n",
    "    sorterC=[\"C1'\", \"N1\", \"C6\", \"C5\", \"C4\", \"N4\",\"N3\", \"C2\", \"O2\"]\n",
    "    sorterG=[\"C1'\", \"N9\", \"C8\", \"N7\", \"C5\", \"C6\", \"O6\", \"N1\",\"C2\", \"N2\",\"N3\", \"C4\"]\n",
    "    sorterU=[\"C1'\", \"N1\", \"C6\",\"C5\",\"C4\", \"O4\", \"N3\",\"C2\", \"O2\"]\n",
    "    sorterIndexA = dict(zip(sorterA,range(len(sorterA))))\n",
    "    sorterIndexC = dict(zip(sorterC,range(len(sorterC))))\n",
    "    sorterIndexG = dict(zip(sorterG,range(len(sorterG))))\n",
    "    sorterIndexU = dict(zip(sorterU,range(len(sorterU))))\n",
    "    pairs=[]\n",
    "    ix=[]\n",
    "    combs=[]\n",
    "    for i in combs_close:\n",
    "        ## Get residue names of the pair\n",
    "        resid1 = mindf.loc[mindf['resno'] == i[0], ['resid']].to_numpy()\n",
    "        resid2 = mindf.loc[mindf['resno'] == i[1], ['resid']].to_numpy()\n",
    "        pair = str(resid1 + resid2)[3:5]\n",
    "        ## If not recognized, swap it\n",
    "        if not pair in listofpairs:\n",
    "            i = (i[1], i[0])\n",
    "            resid1 = mindf.loc[mindf['resno'] == i[0], ['resid']].to_numpy()\n",
    "            resid2 = mindf.loc[mindf['resno'] == i[1], ['resid']].to_numpy()\n",
    "            pair = str(resid1 + resid2)[3:5]\n",
    "        ## Save pair\n",
    "        pairs.append(pair)\n",
    "        ## Get indices for the first base according with pair and desired order of atoms\n",
    "        if resid1 == 'A':\n",
    "            sorterIndex = sorterIndexA\n",
    "        elif resid1 == 'C':\n",
    "            sorterIndex = sorterIndexC\n",
    "        elif resid1 == 'G':\n",
    "            sorterIndex = sorterIndexG\n",
    "        elif resid1 == 'U':\n",
    "            sorterIndex = sorterIndexU\n",
    "        #data_base_1=df2[df2.resno == i[0]]\n",
    "        #ix1=data_base_1.iloc[data_base_1['elety'].map(sorterIndex),].index\n",
    "        data_base_1=df2[df2.resno == i[0]]\n",
    "        data_base_1.insert(len(data_base_1.columns), 'rank', data_base_1['elety'].map(sorterIndex))\n",
    "        ix1=data_base_1.sort_values(['rank']).index\n",
    "        ## Get indices for the second base according with pair and desired order of atoms\n",
    "        if resid2 == 'A':\n",
    "            sorterIndex = sorterIndexA\n",
    "        elif resid2 == 'C':\n",
    "            sorterIndex = sorterIndexC\n",
    "        elif resid2 == 'G':\n",
    "            sorterIndex = sorterIndexG\n",
    "        elif resid2 == 'U':\n",
    "            sorterIndex = sorterIndexU\n",
    "        data_base_2=df2[df2.resno == i[1]]\n",
    "        data_base_2.insert(len(data_base_2.columns), 'rank', data_base_2['elety'].map(sorterIndex))\n",
    "        ix2=data_base_2.sort_values(['rank']).index\n",
    "        #ix1=data_base_1.index[data_base_1.resno == i[0]]\n",
    "        #ix2=data_base_2.index[data_base_2.resno == i[1]]\n",
    "        ix.append([ix1, ix2])\n",
    "        combs.append(list(product(ix1, ix2)))\n",
    "    return [combs_close, pairs, ix, combs, drop]\n",
    "\n",
    "def sorted_alphanumeric(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(data, key=alphanum_key)\n",
    "\n",
    "## Define additional dependencies\n",
    "to_drop = [\"P\", \"OP1\", \"OP2\", \"OP3\", \"HO5'\", \"O5'\", \"C5'\", \"H5'\", \"H5''\", \"C4'\", \"H4'\", \"O4'\", \"H1'\", \"C3'\", \n",
    "           \"H3'\", \"C2'\", \"H2'\", \"O2'\", \"HO2'\", \"O3'\", \"HO3'\", \"H11'\", \"H12'\", \"H13'\",\n",
    "          \"H8\", \"H61\", \"H62\", \"H2\", \"H6\",  \"H5\", \"H41\", \"H42\",  \"H1\",\"H21\",\"H22\", \"H3\"]\n",
    "to_keep = [\"N1\", \"C2\", \"N3\", \"C4\", \"C5\", \"C6\"]\n",
    "listofpairs = ['AA', 'AC', 'AG', 'AU', 'CC', 'CU', 'GC', 'GG', 'GU', 'UU']\n",
    "\n",
    "## Define model filenames\n",
    "#AA_modelfile = pathML + '/AA_model_tpot.sav'\n",
    "#AC_modelfile = pathML + '/AC_model_tpot.sav'\n",
    "#AG_modelfile = pathML + '/AG_model_tpot.sav'\n",
    "#AU_modelfile = pathML + '/AU_model_tpot.sav'\n",
    "#CC_modelfile = pathML + '/CC_model_tpot.sav'\n",
    "#CU_modelfile = pathML + '/CU_model_tpot.sav'\n",
    "#GC_modelfile = pathML + '/GC_model_tpot.sav'\n",
    "#GG_modelfile = pathML + '/GG_model_tpot.sav'\n",
    "#GU_modelfile = pathML + '/GU_model_tpot.sav'\n",
    "#UU_modelfile = pathML + '/UU_model_tpot.sav'\n",
    "AA_modelfile = pathML + '/AA/model_scanned.sav'\n",
    "AC_modelfile = pathML + '/AC/model_scanned.sav'\n",
    "AG_modelfile = pathML + '/AG/model_scanned.sav'\n",
    "AU_modelfile = pathML + '/AU/model_scanned.sav'\n",
    "CC_modelfile = pathML + '/CC/model_scanned.sav'\n",
    "CU_modelfile = pathML + '/CU/model_scanned.sav'\n",
    "GC_modelfile = pathML + '/GC/model_scanned.sav'\n",
    "GG_modelfile = pathML + '/GG/model_scanned.sav'\n",
    "GU_modelfile = pathML + '/GU/model_scanned.sav'\n",
    "UU_modelfile = pathML + '/UU/model_scanned.sav'\n",
    "\n",
    "# Read models from disk\n",
    "AA_model = pickle.load(open(AA_modelfile, 'rb'))\n",
    "AC_model = pickle.load(open(AC_modelfile, 'rb'))\n",
    "AG_model = pickle.load(open(AG_modelfile, 'rb'))\n",
    "AU_model = pickle.load(open(AU_modelfile, 'rb'))\n",
    "CC_model = pickle.load(open(CC_modelfile, 'rb'))\n",
    "CU_model = pickle.load(open(CU_modelfile, 'rb'))\n",
    "GC_model = pickle.load(open(GC_modelfile, 'rb'))\n",
    "GG_model = pickle.load(open(GG_modelfile, 'rb'))\n",
    "GU_model = pickle.load(open(GU_modelfile, 'rb'))\n",
    "UU_model = pickle.load(open(UU_modelfile, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = fnmatch.filter(os.listdir(pathPDB), '*.pdb')\n",
    "files = sorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "structure_000.pdb\n",
      "structure_000.pdb\n",
      "604.6065\n",
      "#########\n",
      "structure_001.pdb\n",
      "structure_001.pdb\n",
      "580.5313\n",
      "#########\n",
      "structure_002.pdb\n",
      "structure_002.pdb\n",
      "528.8363\n",
      "#########\n",
      "structure_003.pdb\n",
      "structure_003.pdb\n",
      "366.0133\n",
      "#########\n",
      "structure_004.pdb\n",
      "structure_004.pdb\n",
      "352.8935\n",
      "#########\n",
      "structure_005.pdb\n",
      "structure_005.pdb\n",
      "71.6559\n",
      "#########\n",
      "structure_006.pdb\n",
      "structure_006.pdb\n",
      "68.187\n",
      "#########\n",
      "structure_007.pdb\n",
      "structure_007.pdb\n",
      "69.474\n",
      "#########\n",
      "structure_008.pdb\n",
      "structure_008.pdb\n",
      "66.434\n",
      "#########\n",
      "structure_009.pdb\n",
      "structure_009.pdb\n",
      "62.6831\n",
      "#########\n",
      "structure_010.pdb\n",
      "structure_010.pdb\n",
      "-439.8154\n",
      "#########\n",
      "structure_011.pdb\n",
      "structure_011.pdb\n",
      "-439.3496\n",
      "#########\n",
      "structure_012.pdb\n",
      "structure_012.pdb\n",
      "-439.4183\n",
      "#########\n",
      "structure_013.pdb\n",
      "structure_013.pdb\n",
      "-479.9186\n",
      "#########\n",
      "structure_014.pdb\n",
      "structure_014.pdb\n",
      "-477.8913\n",
      "#########\n",
      "structure_015.pdb\n",
      "structure_015.pdb\n",
      "-473.5912\n",
      "#########\n",
      "structure_016.pdb\n",
      "structure_016.pdb\n",
      "-1333.0928\n",
      "#########\n",
      "structure_017.pdb\n",
      "structure_017.pdb\n",
      "239.1986\n",
      "#########\n",
      "structure_999.pdb\n",
      "structure_999.pdb\n",
      "601.0042\n",
      "#########\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(files)):\n",
    "    \n",
    "    pdbfile = files[index]\n",
    "    print(pdbfile)\n",
    "    #Gather topology\n",
    "    top = topology(pdbfile=pdbfile, path=pathPDB)\n",
    "    \n",
    "    #Obtain distances and compute energies\n",
    "    df = parsexyz(pdbfile = pdbfile, path = pathPDB)\n",
    "    df = df[~top[4]]\n",
    "    ## Calculate all distances between atoms\n",
    "    dists=cdist(df.iloc[:,], df.iloc[:,], metric='euclidean')\n",
    "    combs_close = top[0]\n",
    "    myseq = list(range(0, len(combs_close), 1))\n",
    "    energies = []\n",
    "    for i in myseq:\n",
    "        pair = top[1][i]\n",
    "        ## Substract desired distances based on presaved indices\n",
    "        distances=dists[top[2][i][0],:][:,top[2][i][1]].flatten()\n",
    "\n",
    "        #Check if distances are lower than 1.5A or higher than 10A\n",
    "        check_low  = np.any(distances < 1.5)\n",
    "        check_up = np.any(distances > 10)\n",
    "\n",
    "        ## Calculate powers\n",
    "        dd = potency2(distances).reshape(1, -1)\n",
    "        modelname = str(pair + '_model.predict(dd)')\n",
    "        #modelname = modelname[3:23]\n",
    "        #print(modelname)\n",
    "        out = round(eval(modelname)[0], 6)\n",
    "        #print(top[0][i], pair, out)\n",
    "        #print(pair)\n",
    "        energies.append(out)\n",
    "    out=round(sum(energies), 4)\n",
    "    \n",
    "    print(pdbfile)\n",
    "    print(out)\n",
    "    print(\"#########\")\n",
    "    \n",
    "    pdb = parsepdb(pdbfile=pdbfile, path=pathPDB)\n",
    "    \n",
    "    #Scoring\n",
    "    scored_pdb = scoring_backbone(top = top, energies = energies, pdb = pdb)\n",
    "    \n",
    "    #Read the pdb\n",
    "    pl1 = PandasPdb().read_pdb(pathPDB + pdbfile)\n",
    "\n",
    "    pl1.df['ATOM'][\"b_factor\"] = scored_pdb[\"B_factor\"]\n",
    "    \n",
    "    \n",
    "    pdbfile_score = pdbfile.split('.pdb')\n",
    "    pdbfile_score = pdbfile_score[0] + '_score.pdb'\n",
    "    path_score = pathPDB + pdbfile_score\n",
    "    \n",
    "    #Store pdb\n",
    "    pl1.to_pdb(path= path_score, \n",
    "                records=None, \n",
    "                gz=False, \n",
    "                append_newline=True)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
